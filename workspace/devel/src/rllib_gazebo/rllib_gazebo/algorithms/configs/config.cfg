# INFO: This file is a collection of default configs used for RLLib.

# ---------------------> GENERIC

RESOURCES = {
    'num_gpus': 1, # read from argparse (--cpu_only)
    'num_cpus_per_worker': 1,
    'num_gpus_per_worker': 1,
    'num_learner_workers': 0,
}

FRAMEWORK = {
    'framework_str': 'tf2',
    'eager_tracing': False,
}

ENVIRONMENT = {
    'env': None, # override later
    'env_config': {}, # override later
    'observation_space': None,
    'action_space': None,
    'env_task_fn': None, # override later
    'render_env': False,
    'clip_rewards': False,
    'normalize_actions': False,
    'clip_actions': False,
}

ROLLOUTS = {
    'num_rollout_workers': 0,
    'num_envs_per_worker': 1,
    'sample_async': False,
    'enable_connectors': True,
    'use_worker_filter_stats': True,
    'update_worker_filter_stats': True,
    'rollout_fragment_length': 'auto',
    'batch_mode': 'truncate_episodes',
    'preprocessor_pref': None,
    'observation_filter': 'NoFilter',
    'compress_observations': True,
}

TRAINING = {
    'gamma': 0.95, # read from argparse
    'lr': 1e-04, # read from argparse
    'grad_clip': None,
    'grad_clip_by': 'global_norm',
    'train_batch_size': 32, # read from argparse
    'model': None, # override later
}

CALLBACKS = {
    'callbacks_class': None, # override later
}

EXPLORATION = {
    'explore': True, # read from argparse
    'exploration_config': {}, # override later
}

EVALUATION = {
    'evaluation_interval': None,
    'evaluation_duration': 10, # read from argparse
    'evaluation_duration_unit': 'episodes', # read from argparse
    'evaluation_parallel_to_training': False,
    'evaluation_config': {}, # override later (copy of train, but without exploration)
    'evaluation_num_workers': 0,
    'custom_evaluation_function': None,
    'always_attach_evaluation_results': False,
    'enable_async_evaluation': False,
}

REPORTING = {
    'keep_per_episode_custom_metrics': False,
    'metrics_num_episodes_for_smoothing': 10,
    'min_train_timesteps_per_iteration': None, # ???
    'min_sample_timesteps_per_iteration': None, # ???
}

CHECKPOINTING = {
    'export_native_model_files': False,
    'checkpoint_trainable_policies_only': True,
}

DEBUGGING = {
    'log_level': 'WARN' # read from argparse (--verbosity)
}

FAULT_TOLERANCE = {
    'recreate_failed_workers': True,
    'max_num_worker_restarts': 3,
    'delay_between_worker_restarts_s': 10,
}

# ---------------------> PG

VPG = {
    'lr_schedule': None,
    'lr': 0.0004,
    'rollout_fragment_length': 1000,
    'train_batch_size': 128,
    '_disable_preprocessor_api': True,
}

SS = {
    'type': 'StochasticSampling',
    'random_timesteps': 0,
}

PN = {
    'type': 'ParameterNoise',
    'initial_stddev': 1.0,
    'random_timesteps': 10000,
}

# ---------------------> DQN
# ---------> BUFFER

MARB = {
    "_enable_replay_buffer_api": True,
    "type": "MultiAgentReplayBuffer",
    "capacity": 50000,
    "replay_sequence_length": 1,
}

MAPRB = {
    'type': 'MultiAgentPrioritizedReplayBuffer',
    'capacity': 50000,
    'prioritized_replay_alpha': 0.6,
    'prioritized_replay_beta': 0.4,
    'prioritized_replay_eps': 1e-06,
    'replay_sequence_length': 1,
    'worker_side_prioritization': False,
}
